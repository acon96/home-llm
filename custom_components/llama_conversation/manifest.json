{
  "domain": "llama_conversation",
  "name": "Local LLMs",
  "version": "0.4.6",
  "codeowners": ["@acon96"],
  "config_flow": true,
  "dependencies": ["conversation", "ai_task"],
  "after_dependencies": ["assist_pipeline", "intent"],
  "documentation": "https://github.com/acon96/home-llm",
  "integration_type": "service",
  "iot_class": "local_polling",
  "requirements": [
      "huggingface-hub>=0.23.0",
      "webcolors>=24.8.0",
      "ollama>=0.5.1",
      "anthropic>=0.75.0"
  ]
}
