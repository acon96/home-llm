{
    "config": {
        "error": {
            "download_failed": "The download failed to complete!",
            "failed_to_connect": "Failed to connect to the remote API. See the logs for more details.",
            "missing_model_api": "The selected model is not provided by this API.",
            "missing_model_file": "The provided file does not exist.",
            "other_existing_local": "Another model is already loaded locally. Please unload it or configure a remote model.",
            "unknown": "Unexpected error",
            "missing_wheels": "Llama.cpp is not installed and could not find any wheels to install!",
            "pip_wheel_error": "Pip returned an error while installing the wheel!"
        },
        "progress": {
            "download": "Please wait while the model is being downloaded from HuggingFace. This can take a few minutes.",
            "install_local_wheels": "Please wait while Llama.cpp is installed..."
        },
        "step": {
            "local_model": {
                "data": {
                    "downloaded_model_file": "Local file name",
                    "downloaded_model_quantization": "Downloaded model quantization",
                    "huggingface_model": "HuggingFace Model"
                },
                "description": "Please configure llama.cpp for the model",
                "title": "Configure llama.cpp"
            },
            "remote_model": {
                "data": {
                    "host": "API Hostname",
                    "huggingface_model": "Model Name",
                    "port": "API Port",
                    "openai_api_key": "API Key",
                    "text_generation_webui_admin_key": "Admin Key",
                    "text_generation_webui_preset": "Generation Preset/Character Name",
                    "remote_use_chat_endpoint": "Use chat completions endpoint",
                    "text_generation_webui_chat_mode": "Chat Mode"
                },
                "description": "Provide the connection details for an instance of text-generation-webui that is hosting the model.",
                "title": "Configure connection to remote API"
            },
            "pick_backend": {
                "data": {
                    "download_model_from_hf": "Download model from HuggingFace",
                    "use_local_backend": "Use Llama.cpp"
                },
                "description": "Select the backend for running the model. The options are:\n1. Llama.cpp with a model from HuggingFace\n2. Llama.cpp with a model stored on the disk\n3. [text-generation-webui API](https://github.com/oobabooga/text-generation-webui)\n4. Generic OpenAI API Compatible API\n5. [llama-cpp-python Server](https://llama-cpp-python.readthedocs.io/en/latest/server/)\n6. [Ollama API](https://github.com/jmorganca/ollama/blob/main/docs/api.md)\n\nIf using Llama.cpp locally, make sure you copied the correct wheel file to the same directory as the integration.",
                "title": "Select Backend"
            }
        }
    },
    "options": {
        "step": {
            "init": {
                "data": {
                    "max_new_tokens": "Maximum tokens to return in response",
                    "prompt": "System Prompt",
                    "prompt_template": "Prompt Format",
                    "temperature": "Temperature",
                    "top_k": "Top K",
                    "top_p": "Top P",
                    "request_timeout": "Remote Request Timeout (seconds)",
                    "extra_attributes_to_expose": "Additional attribute to expose in the context",
                    "gbnf_grammar": "Enable GBNF Grammar",
                    "openai_api_key": "API Key",
                    "text_generation_webui_admin_key": "Admin Key",
                    "service_call_regex": "Service Call Regex",
                    "refresh_prompt_per_tern": "Refresh System Prompt Every Turn",
                    "text_generation_webui_preset": "Generation Preset/Character Name",
                    "remote_use_chat_endpoint": "Use chat completions endpoint",
                    "text_generation_webui_chat_mode": "Chat Mode"
                }
            }
        }
    },
    "selector": {
        "prompt_template": {
            "options": {
                "chatml": "ChatML",
                "vicuna": "Vicuna",
                "alpaca": "Alpaca",
                "mistral": "Mistral",
                "no_prompt_template": "None"
            }
        },
        "model_backend": {
            "options": {
                "llama_cpp_hf": "Llama.cpp (HuggingFace)",
                "llama_cpp_existing": "Llama.cpp (existing model)",
                "text-generation-webui_api": "text-generation-webui API",
                "generic_openai": "Generic OpenAI Compatible API",
                "llama_cpp_python_server": "llama-cpp-python Server",
                "ollama": "Ollama API"

            }
        },
        "text_generation_webui_chat_mode": {
            "options": {
                "chat": "Chat",
                "instruct": "Instruct",
                "chat-instruct": "Chat-Instruct"
            }
        }
    }
}