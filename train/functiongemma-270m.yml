base_model: google/functiongemma-270m-it
model_type: Gemma3ForCausalLM

# gemma3 doesn't seem to play nice with ddp
ddp_find_unused_parameters: true

datasets:
  - path: /workspace/data/datasets/sample.jsonl
    ds_type: json
    type: chat_template
    roles_to_train:
      - assistant

val_set_size: 0.0
output_dir: /workspace/data/training-runs/Home-Gemma3-270m

sequence_len: 4096
sample_packing: true
eval_sample_packing: false

use_tensorboard: true

# batch size = 16
gradient_accumulation_steps: 16
micro_batch_size: 1
num_epochs: 1
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002

bf16: true

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
resume_from_checkpoint:
logging_steps: 1
flash_attention: true

warmup_ratio: 0.1
evals_per_epoch:
saves_per_epoch: 1
weight_decay: 0.0
